{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc5e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420a458a",
   "metadata": {},
   "source": [
    "# three_layer_neuron_network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    '''\n",
    "    generate data\n",
    "    :return: X: input data, y: given labels\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    X, y = datasets.make_moons(200, noise=0.20)\n",
    "    return X, y\n",
    "\n",
    "def plot_decision_boundary(pred_func, X, y):\n",
    "    '''\n",
    "    plot the decision boundary\n",
    "    :param pred_func: function used to predict the label\n",
    "    :param X: input data\n",
    "    :param y: given labels\n",
    "    :return:\n",
    "    '''\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "# YOUR ASSSIGMENT STARTS HERE\n",
    "# FOLLOW THE INSTRUCTION BELOW TO BUILD AND TRAIN A 3-LAYER NEURAL NETWORK\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    This class builds and trains a neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, nn_input_dim, nn_hidden_dim , nn_output_dim, actFun_type='tanh', reg_lambda=0.01, seed=0):\n",
    "        '''\n",
    "        :param nn_input_dim: input dimension\n",
    "        :param nn_hidden_dim: the number of hidden units\n",
    "        :param nn_output_dim: output dimension\n",
    "        :param actFun_type: type of activation function. 3 options: 'tanh', 'sigmoid', 'relu'\n",
    "        :param reg_lambda: regularization coefficient\n",
    "        :param seed: random seed\n",
    "        '''\n",
    "        self.nn_input_dim = nn_input_dim\n",
    "        self.nn_hidden_dim = nn_hidden_dim\n",
    "        self.nn_output_dim = nn_output_dim\n",
    "        self.actFun_type = actFun_type\n",
    "        self.reg_lambda = reg_lambda\n",
    "        \n",
    "        # initialize the weights and biases in the network\n",
    "        np.random.seed(seed)\n",
    "        self.W1 = np.random.randn(self.nn_input_dim, self.nn_hidden_dim) / np.sqrt(self.nn_input_dim)\n",
    "        self.b1 = np.zeros((1, self.nn_hidden_dim))\n",
    "        self.W2 = np.random.randn(self.nn_hidden_dim, self.nn_output_dim) / np.sqrt(self.nn_hidden_dim)\n",
    "        self.b2 = np.zeros((1, self.nn_output_dim))\n",
    "\n",
    "    def actFun(self, z, type):\n",
    "        '''\n",
    "        actFun computes the activation functions\n",
    "        :param z: net input\n",
    "        :param type: tanh, sigmoid, or relu\n",
    "        :return: activations\n",
    "        '''\n",
    "\n",
    "        # YOU IMPLMENT YOUR actFun HERE\n",
    "        if type.lower()=='tanh':\n",
    "            z_out = np.tanh(z)\n",
    "        elif type.lower()=='sigmoid':\n",
    "            z_out = 1 / (1 + np.exp(-z))\n",
    "        elif type.lower()=='relu':\n",
    "            z_out = np.maximum(0, z)\n",
    "        else:\n",
    "            raise ValueError('That activation function type is not defined.')\n",
    "\n",
    "        return z_out\n",
    "\n",
    "    def diff_actFun(self, z, type):\n",
    "        '''\n",
    "        diff_actFun computes the derivatives of the activation functions wrt the net input\n",
    "        :param z: net input\n",
    "        :param type: Tanh, Sigmoid, or ReLU\n",
    "        :return: the derivatives of the activation functions wrt the net input\n",
    "        '''\n",
    "\n",
    "        # YOU IMPLEMENT YOUR diff_actFun HERE\n",
    "        if type.lower()=='tanh':\n",
    "            z_out_prime = 1 - np.power(np.tanh(z), 2)\n",
    "        elif type.lower()=='sigmoid':\n",
    "            z_out_prime = self.actFun(z, type)*(1 - self.actFun(z, type))\n",
    "        elif type.lower()=='relu':\n",
    "            z_out_prime = (z > 0).astype('float')\n",
    "        else:\n",
    "            raise ValueError('That activation function type is not defined.')\n",
    "\n",
    "        return z_out_prime\n",
    "\n",
    "    def feedforward(self, X, actFun):\n",
    "        '''\n",
    "        feedforward builds a 3-layer neural network and computes the two probabilities,\n",
    "        one for class 0 and one for class 1\n",
    "        :param X: input data\n",
    "        :param actFun: activation function\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        # YOU IMPLEMENT YOUR feedforward HERE\n",
    "        self.z1 = X@self.W1 + self.b1\n",
    "        self.a1 = actFun(self.z1)\n",
    "        self.z2 = self.a1@self.W2 + self.b2\n",
    "        # No a2? Ig implicitly the below is analogous to softmax?\n",
    "        exp_scores = np.exp(self.z2)\n",
    "        self.probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return None\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        '''\n",
    "        calculate_loss computes the loss for prediction\n",
    "        :param X: input data\n",
    "        :param y: given labels\n",
    "        :return: the loss for prediction\n",
    "        '''\n",
    "        num_examples = len(X)\n",
    "        self.feedforward(X, lambda x: self.actFun(x, type=self.actFun_type))\n",
    "        # YOU IMPLEMENT YOUR CALCULATION OF THE LOSS HERE\n",
    "        y_onehot = OneHotEncoder(sparse_output=False).fit_transform(y.reshape((-1, 1)))\n",
    "        data_loss = (-1/num_examples) * np.sum(np.log(self.probs) * y_onehot)\n",
    "\n",
    "        # Add regulatization term to loss (optional)\n",
    "        data_loss += self.reg_lambda / 2 * (np.sum(np.square(self.W1)) + np.sum(np.square(self.W2)))\n",
    "        return (1. / num_examples) * data_loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        predict infers the label of a given data point X\n",
    "        :param X: input data\n",
    "        :return: label inferred\n",
    "        '''\n",
    "        self.feedforward(X, lambda x: self.actFun(x, type=self.actFun_type))\n",
    "        return np.argmax(self.probs, axis=1)\n",
    "\n",
    "    def backprop(self, X, y):\n",
    "        '''\n",
    "        backprop implements backpropagation to compute the gradients used to update the parameters in the backward step\n",
    "        :param X: input data\n",
    "        :param y: given labels\n",
    "        :return: dL/dW1, dL/b1, dL/dW2, dL/db2\n",
    "        '''\n",
    "\n",
    "        # IMPLEMENT YOUR BACKPROP HERE\n",
    "        num_examples = len(X)\n",
    "        delta3 = self.probs\n",
    "        delta3[range(num_examples), y] -= 1\n",
    "\n",
    "        # dW2 = dL/dW2\n",
    "        dW2 = np.dot(self.a1.T, delta3)\n",
    "        # db2 = dL/db2\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        # Need to find delta2 for earlier layer\n",
    "        diff = self.diff_actFun(self.z1, type=self.actFun_type) # Should z1 be a1?\n",
    "        delta2 = np.dot(delta3, self.W2.T) * diff\n",
    "        # dW1 = dL/dW1\n",
    "        dW1 = np.dot(X.T, delta2)\n",
    "        # db1 = dL/db1\n",
    "        db1 = np.sum(delta2, axis=0) #, keepdims=False\n",
    "\n",
    "        return dW1, dW2, db1, db2\n",
    "\n",
    "    def fit_model(self, X, y, epsilon=0.01, num_passes=20000, print_loss=True):\n",
    "        '''\n",
    "        fit_model uses backpropagation to train the network\n",
    "        :param X: input data\n",
    "        :param y: given labels\n",
    "        :param num_passes: the number of times that the algorithm runs through the whole dataset\n",
    "        :param print_loss: print the loss or not\n",
    "        :return:\n",
    "        '''\n",
    "        # Gradient descent.\n",
    "        for i in range(0, num_passes):\n",
    "            # Forward propagation\n",
    "            self.feedforward(X, lambda x: self.actFun(x, type=self.actFun_type))\n",
    "            # Backpropagation\n",
    "            dW1, dW2, db1, db2 = self.backprop(X, y)\n",
    "\n",
    "            # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "            dW2 += self.reg_lambda * self.W2\n",
    "            dW1 += self.reg_lambda * self.W1\n",
    "\n",
    "            # Gradient descent parameter update\n",
    "            self.W1 += -epsilon * dW1\n",
    "            self.b1 += -epsilon * db1\n",
    "            self.W2 += -epsilon * dW2\n",
    "            self.b2 += -epsilon * db2\n",
    "\n",
    "            # Optionally print the loss.\n",
    "            # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "            if print_loss and i % 1000 == 0:\n",
    "                print(\"Loss after iteration %i: %f\" % (i, self.calculate_loss(X, y)))\n",
    "\n",
    "    def visualize_decision_boundary(self, X, y):\n",
    "        '''\n",
    "        visualize_decision_boundary plots the decision boundary created by the trained network\n",
    "        :param X: input data\n",
    "        :param y: given labels\n",
    "        :return:\n",
    "        '''\n",
    "        plot_decision_boundary(lambda x: self.predict(x), X, y)\n",
    "\n",
    "def main():\n",
    "    # # generate and visualize Make-Moons dataset\n",
    "    X, y = generate_data()\n",
    "    #####################################################\n",
    "    ### INCLUDE THIS FIGURE IN THE REPORT ###\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=40, c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()\n",
    "\n",
    "    # tanh, sigmoid, relu\n",
    "    model = NeuralNetwork(nn_input_dim=2, nn_hidden_dim=3 , nn_output_dim=2, actFun_type='tanh')\n",
    "    model.fit_model(X,y)\n",
    "    model.visualize_decision_boundary(X,y)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a05d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate and visualize Make-Moons dataset\n",
    "X, y = generate_data()\n",
    "#####################################################\n",
    "### INCLUDE THIS FIGURE IN THE REPORT ###\n",
    "plt.scatter(X[:, 0], X[:, 1], s=40, c=y, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh, sigmoid, relu\n",
    "model = NeuralNetwork(nn_input_dim=2, nn_hidden_dim=3 , nn_output_dim=2, actFun_type='tanh')\n",
    "model.fit_model(X,y)\n",
    "model.visualize_decision_boundary(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4430d17d",
   "metadata": {},
   "source": [
    "Train the network using different activation functions (Tanh, Sigmoid and ReLU). Describe and explain the differences that you observe. Include the figures generated in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cf37f3",
   "metadata": {},
   "source": [
    "tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(nn_input_dim=2, nn_hidden_dim=3 , nn_output_dim=2, actFun_type='tanh')\n",
    "model.fit_model(X,y)\n",
    "model.visualize_decision_boundary(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5379d8",
   "metadata": {},
   "source": [
    "sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01842682",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(nn_input_dim=2, nn_hidden_dim=3 , nn_output_dim=2, actFun_type='sigmoid')\n",
    "model.fit_model(X,y)\n",
    "model.visualize_decision_boundary(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bb2d04",
   "metadata": {},
   "source": [
    "relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb03c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(nn_input_dim=2, nn_hidden_dim=3 , nn_output_dim=2, actFun_type='relu')\n",
    "model.fit_model(X,y)\n",
    "model.visualize_decision_boundary(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36fea2d",
   "metadata": {},
   "source": [
    "> __Conclusions:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f689dd",
   "metadata": {},
   "source": [
    "Increase the number of hidden units (nn hidden dim) and retrain the network using Tanh as the activation function. Describe and explain the differences that you observe. Include the figures generated in your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c196532",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(nn_input_dim=2, nn_hidden_dim=10 , nn_output_dim=2, actFun_type='tanh')\n",
    "model.fit_model(X,y)\n",
    "model.visualize_decision_boundary(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cadc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(nn_input_dim=2, nn_hidden_dim=100 , nn_output_dim=2, actFun_type='tanh')\n",
    "model.fit_model(X,y)\n",
    "model.visualize_decision_boundary(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28b9313",
   "metadata": {},
   "source": [
    "> __Conclusions:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53580fb9",
   "metadata": {},
   "source": [
    "# n_layer_neural_network.py\n",
    "- We provide you hints below to help you organize and implement the code, but if you have better ideas, please feel free to implement them and ignore our hints. In your report, please tell us why you made the choice(s) you did.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0990fd25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
